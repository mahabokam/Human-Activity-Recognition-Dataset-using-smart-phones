# -*- coding: utf-8 -*-
"""Human Activity Recognition Dataset using smart phones

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1McwgSg9bPDOomYaJagxt8IrWNOSiksId
"""

from google.colab import drive
drive.mount('/content/drive')

pip install tensorflow scikit-learn pandas numpy

!pip install scikeras

from scikeras.wrappers import KerasRegressor

import pandas as pd

file_path = '/content/drive/MyDrive/dataset/train.csv'
data = pd.read_csv(file_path)

# Display the first few rows
print(data.head())

import pandas as pd

# Path to the *extracted* csv file
file_path = '/content/drive/MyDrive/dataset/test.csv'

data = pd.read_csv(file_path)

print(data.head())

# Import necessary libraries
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import BaggingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Import Keras and SciKeras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from scikeras.wrappers import KerasRegressor  # Ensure scikeras is installed

import matplotlib.pyplot as plt

import os
import pandas as pd

# Check if the file exists
file_path = '/content/drive/MyDrive/dataset/test.csv'
print("File exists:", os.path.exists(file_path))

# Load the data if the file exists
if os.path.exists(file_path):
    data = pd.read_csv(file_path)
    print(data.head())  # Display the first 5 rows to verify the data
else:
    print(f"Error: File '{file_path}' not found.")

import os
import pandas as pd

# Check if the file exists
file_path = '/content/drive/MyDrive/dataset/train.csv'
print("File exists:", os.path.exists(file_path))

# Load the data if the file exists
if os.path.exists(file_path):
    data = pd.read_csv(file_path)
    print(data.head())  # Display the first 5 rows to verify the data
else:
    print(f"Error: File '{file_path}' not found.")

import pandas as pd

# Load train and test datasets
train_data = pd.read_csv("/content/drive/MyDrive/dataset/train.csv")
test_data = pd.read_csv("/content/drive/MyDrive/dataset/test.csv")

# Display basic info
print("Train Data Shape:", train_data.shape)
print("Test Data Shape:", test_data.shape)

# # Ensure 'Activity' and 'subject' are treated as categorical
# train_data["Activity"] = train_data["Activity"].astype("category")
# train_data["subject"] = train_data["subject"].astype("category")
# test_data["Activity"] = test_data["Activity"].astype("category")
# test_data["subject"] = test_data["subject"].astype("category")

# # Preview Data
# print(train_data.head())
# print(test_data.head())

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Define the ANN model, accepting the input dimension as an argument
def create_ann(input_dim):
    model = Sequential()

    # Use the input_dim argument here
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1))

    model.compile(optimizer='adam', loss='mse')
    return model

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# --- Step 1: Load and Prepare Your Data ---
# Replace this section with the code to load your actual dataset.
# This is just a placeholder to make the example runnable.
X_sample = np.random.rand(100, 10) # 100 samples, 10 features
y_sample = np.random.rand(100, 1)  # 100 target values

# --- Step 2: Split Data into Training and Testing Sets ---
# This is the crucial step that creates the X_train variable.
X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)

# --- Step 3: Define Your Model Creation Function ---
def create_ann(input_dim):
    model = Sequential()
    model.add(Dense(128, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# --- Step 4: Create the Model ---
# Now that X_train exists, this line will work correctly.
input_dimension = X_train.shape[1]
ann = create_ann(input_dimension)

# --- Step 5: Check the Summary ---
print("Model Summary:")
ann.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from scikeras.wrappers import KerasRegressor
from sklearn.ensemble import BaggingRegressor

# Define the ANN model function
def create_ann():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1))  # Output layer for regression

    # Compile the model
    model.compile(optimizer='adam', loss='mse')
    return model

# Wrap Keras model with KerasRegressor
ann_wrapper = KerasRegressor(model=create_ann, epochs=50, batch_size=32, verbose=0)

# Create BaggingRegressor with the wrapped model
bagging_model = BaggingRegressor(estimator=ann_wrapper, n_estimators=2, random_state=42)

# Train the model
bagging_model.fit(X_train, y_train)

import tensorflow as tf

# Set a global random seed for reproducibility
# You can use any integer you like for the seed.
tf.random.set_seed(42)

# === IMPORTS ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# === SETUP ===
tf.random.set_seed(42)  # for reproducibility
sns.set(style='whitegrid')
plt.rcParams["figure.figsize"] = (10, 6)

# === STEP 1: LOAD DATA ===
data_path = '/content/drive/MyDrive/dataset/'

print("Loading dataset...")
train_df = pd.read_csv(data_path + 'train.csv')
test_df = pd.read_csv(data_path + 'test.csv')

# === STEP 2: SPLIT FEATURES & LABELS ===
X_train = train_df.iloc[:, :-1]
y_train = train_df.iloc[:, -1]
X_test = test_df.iloc[:, :-1]
y_test = test_df.iloc[:, -1]

# === STEP 3: PREPROCESS ===
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

num_classes = len(np.unique(y_train))

# === STEP 4: BUILD MODEL ===
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# === STEP 5: TRAIN MODEL ===
print("\nTraining model...")
history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2, verbose=1)
print("Training complete.")

# === STEP 6: PLOT TRAINING HISTORY ===
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

# === STEP 7: EVALUATE ON TEST SET ===
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… Test Accuracy: {accuracy * 100:.2f}%")

y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Create label names if not available
activity_labels = [f'Class {i}' for i in range(num_classes)]

# === STEP 8: REPORT ===
print("\nðŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, target_names=activity_labels))

# === STEP 9: CONFUSION MATRIX ===
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=activity_labels, yticklabels=activity_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - HAR Dataset")
plt.show()